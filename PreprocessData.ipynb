{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ModelNet dataset. Support ModelNet40, ModelNet10, XYZ and normal channels. Up to 10000 points.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "class ModelNetDataset():\n",
    "    def __init__(self, root, batch_size = 32, npoints = 1024, split='train', normalize=True, normal_channel=True, modelnet10=False, cache_size=15000, shuffle=None):\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.npoints = npoints\n",
    "        self.normalize = normalize\n",
    "        if modelnet10:\n",
    "            self.catfile = os.path.join(self.root, 'modelnet10_shape_names.txt')\n",
    "        else:\n",
    "            self.catfile = os.path.join(self.root, 'modelnet40_shape_names.txt')\n",
    "        self.cat = [line.rstrip() for line in open(self.catfile)]\n",
    "        self.classes = dict(zip(self.cat, range(len(self.cat))))  \n",
    "        self.normal_channel = normal_channel\n",
    "        \n",
    "        shape_ids = {}\n",
    "        if modelnet10:\n",
    "            shape_ids['train'] = [line.rstrip() for line in open(os.path.join(self.root, 'modelnet10_train.txt'))] \n",
    "            shape_ids['test']= [line.rstrip() for line in open(os.path.join(self.root, 'modelnet10_test.txt'))]\n",
    "        else:\n",
    "            shape_ids['train'] = [line.rstrip() for line in open(os.path.join(self.root, 'modelnet40_train.txt'))] \n",
    "            shape_ids['test']= [line.rstrip() for line in open(os.path.join(self.root, 'modelnet40_test.txt'))]\n",
    "        assert(split=='train' or split=='test')\n",
    "        shape_names = ['_'.join(x.split('_')[0:-1]) for x in shape_ids[split]]\n",
    "        # list of (shape_name, shape_txt_file_path) tuple\n",
    "        self.datapath = [(shape_names[i], os.path.join(self.root, shape_names[i], shape_ids[split][i])+'.txt') for i in range(len(shape_ids[split]))]\n",
    "\n",
    "        self.cache_size = cache_size # how many data points to cache in memory\n",
    "        self.cache = {} # from index to (point_set, cls) tuple\n",
    "\n",
    "        if shuffle is None:\n",
    "            if split == 'train': self.shuffle = True\n",
    "            else: self.shuffle = False\n",
    "        else:\n",
    "            self.shuffle = shuffle\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _augment_batch_data(self, batch_data):\n",
    "        if self.normal_channel:\n",
    "            rotated_data = provider.rotate_point_cloud_with_normal(batch_data)\n",
    "            rotated_data = provider.rotate_perturbation_point_cloud_with_normal(rotated_data)\n",
    "        else:\n",
    "            rotated_data = provider.rotate_point_cloud(batch_data)\n",
    "            rotated_data = provider.rotate_perturbation_point_cloud(rotated_data)\n",
    "    \n",
    "        jittered_data = provider.random_scale_point_cloud(rotated_data[:,:,0:3])\n",
    "        jittered_data = provider.shift_point_cloud(jittered_data)\n",
    "        jittered_data = provider.jitter_point_cloud(jittered_data)\n",
    "        rotated_data[:,:,0:3] = jittered_data\n",
    "        return provider.shuffle_points(rotated_data)\n",
    "\n",
    "\n",
    "    def _get_item(self, index): \n",
    "        if index in self.cache:\n",
    "            point_set, cls = self.cache[index]\n",
    "        else:\n",
    "            fn = self.datapath[index]\n",
    "            cls = self.classes[self.datapath[index][0]]\n",
    "            cls = np.array([cls]).astype(np.int32)\n",
    "            point_set = np.loadtxt(fn[1],delimiter=',').astype(np.float32)\n",
    "            # Take the first npoints\n",
    "            point_set = point_set[0:self.npoints,:]\n",
    "            if self.normalize:\n",
    "                point_set[:,0:3] = pc_normalize(point_set[:,0:3])\n",
    "            if not self.normal_channel:\n",
    "                point_set = point_set[:,0:3]\n",
    "            if len(self.cache) < self.cache_size:\n",
    "                self.cache[index] = (point_set, cls)\n",
    "        return point_set, cls\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self._get_item(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n",
    "    def num_channel(self):\n",
    "        if self.normal_channel:\n",
    "            return 6\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    def reset(self):\n",
    "        self.idxs = np.arange(0, len(self.datapath))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "        self.num_batches = (len(self.datapath)+self.batch_size-1) // self.batch_size\n",
    "        self.batch_idx = 0\n",
    "\n",
    "    def has_next_batch(self):\n",
    "        return self.batch_idx < self.num_batches\n",
    "\n",
    "    def next_batch(self, augment=False):\n",
    "        ''' returned dimension may be smaller than self.batch_size '''\n",
    "        start_idx = self.batch_idx * self.batch_size\n",
    "        end_idx = min((self.batch_idx+1) * self.batch_size, len(self.datapath))\n",
    "        bsize = end_idx - start_idx\n",
    "        batch_data = np.zeros((bsize, self.npoints, self.num_channel()))\n",
    "        batch_label = np.zeros((bsize), dtype=np.int32)\n",
    "        for i in range(bsize):\n",
    "            ps,cls = self._get_item(self.idxs[i+start_idx])\n",
    "            batch_data[i] = ps\n",
    "            batch_label[i] = cls\n",
    "        self.batch_idx += 1\n",
    "        if augment: batch_data = self._augment_batch_data(batch_data)\n",
    "        return batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9843, 1024, 6)\n",
      "(9843, 1024, 6)\n"
     ]
    }
   ],
   "source": [
    "d = ModelNetDataset(\"modelnet40_normal_resampled/\",split=\"train\",normal_channel=True)\n",
    "train_data = np.zeros((len(d),1024,6))\n",
    "train_label = np.zeros((len(d)))\n",
    "print(train_data.shape)\n",
    "for i in range(len(d)):\n",
    "    pc,label = d[i]\n",
    "    train_data[i] = pc\n",
    "    train_label[i] = label\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2468, 1024, 6)\n",
      "(2468, 1024, 6)\n"
     ]
    }
   ],
   "source": [
    "d = ModelNetDataset(\"modelnet40_normal_resampled/\",split=\"test\",normal_channel=True)\n",
    "test_data = np.zeros((len(d),1024,6))\n",
    "test_label = np.zeros((len(d)))\n",
    "print(test_data.shape)\n",
    "for i in range(len(d)):\n",
    "    pc,label = d[i]\n",
    "    test_data[i] = pc\n",
    "    test_label[i] = label\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"cls_data_train.npy\",train_data)\n",
    "# np.save(\"cls_label_train.npy\",train_label)\n",
    "np.save(\"cls_data_test.npy\",test_data)\n",
    "np.save(\"cls_label_test.npy\",test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
